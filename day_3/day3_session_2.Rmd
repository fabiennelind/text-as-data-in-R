---
title: "day3_supervised"
author: "Fabienne Lind & Petro Tolochko"
date: "2022-11-12"
output: html_document
---



## Load the corpus

We use a labeled Movie Review Dataset to implement a simple supervised machine learning approach. The dataset contains 5,331 positive and 5,331 negative processed sentences from Rotten Tomatoes movie reviews.
Our goal is to train a classifier that can predict whether a sentence is positive or negative.

This data was first used in Bo Pang and Lillian Lee, ``Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.'', Proceedings of the ACL, 2005. Please find more information on the dataset here.
https://huggingface.co/datasets/rotten_tomatoes

```{r}
library(data.table)
#setwd("/Users/fabiennelind/ucloud/Lehre/PhD course text analysis/R scripts")
#reviews <- fread("rotten_tomatoes.csv", stringsAsFactors=F)


reviews <- read.csv("https://raw.githubusercontent.com/fabiennelind/text-as-data-in-R/main/data/rotten_tomatoes.csv")

```

## Inspect the data


```{r}
colnames(reviews)
names(reviews)[names(reviews) == 'value'] <- 'text' #give the column with the text a useful name
class(reviews$text)

table(reviews$polarity) #check the distribution of the outcome variable
class(reviews$polarity)

```

We now work with the R package quanteda. Please check out the following tutorials.
https://quanteda.io/articles/quickstart.html
https://content-analysis-with-r.com/5-machine_learning.html
https://tutorials.quanteda.io/basic-operations/corpus/corpus/


Create a corpus and look at a summary.

```{r}
library(quanteda)
reviews_corpus <- corpus(reviews)
summary(reviews_corpus, 5)
```


```{r}
# create docvar with ID
reviews_corpus$id_numeric <- 1:ndoc(reviews_corpus)
summary(reviews_corpus, 5)
```


Create a vector which includes the ids for the training part (here 80%) and for the test data (here 20%).
We randomly select the 80%. The remaining reviews are assigned as test cases.
Once we have the DFM, we split it into training and test set. We'll go with 80% training and 20% set. Note the use of a random seed to make sure our results are replicable.


```{r}
set.seed(674)
id_train <- sample(1:nrow(reviews), floor(.80 * nrow(reviews)))
id_test <- (1:nrow(reviews))[1:nrow(reviews) %in% id_train == FALSE]
```



```{r}
# tokenize texts and represent as dfm
toks_reviews <- tokens(reviews_corpus, remove_punct = TRUE, remove_number = TRUE) %>% 
               tokens_remove(pattern = stopwords("en")) %>% 
               tokens_wordstem()
dfm_reviews <- dfm(toks_reviews)
dfm_reviews
```

```{r}
dfm_reviews_trim <- dfm_trim(dfm_reviews, min_docfreq = 2, verbose=TRUE) 
dfm_reviews_trim
```

Split the dfm in two parts, a training and a test part.


```{r}
# get training set
dfm_train <- dfm_subset(dfm_reviews_trim, id_numeric %in% id_train)

# get test set (documents not in id_train)
dfm_test <- dfm_subset(dfm_reviews_trim, !id_numeric %in% id_train)

```

#Training

Fit a Naïve Bayes Classifier on the training dfm and save the learned model in the object 'model.NB'.
The Naïve Bayes Classifier is part of the library quanteda.textmodels. 

```{r}
library(quanteda.textmodels)
model.NB <- textmodel_nb(dfm_train, dfm_train$polarity, prior = "docfreq")

```


Fit a Linear SVM classifier on the training dfm and save the learned model in the object 'model.svm'.

```{r}
library(quanteda.textmodels)
model.svm <- textmodel_svm(dfm_training, dfm_training$polarity, prior = "docfreq")

```

#Predict for the test set

```{r}
pred.nb <- predict(model.NB, dfm_test, force = TRUE) # force = True will force your test data to give identical features (and ordering of features) to the training set

```

```{r}
summary(pred.nb)
```


```{r}
pred.svm <- predict(model.svm, dfm_test, force = TRUE) # force = True will force your test data to give identical features (and ordering of features) to the training set

```

Add the labels predicted by the model to the initial dataframe. Name the new column polarity_ml.


```{r}
colnames(reviews)

reviews$id <- 1:nrow(reviews)
reviews_test <- subset(reviews, id %in% id_test)
reviews_test$polarity_ml <- pred.nb
colnames(reviews_test)
```


## Compare automated with manual classifications 

We compare the automated classification (in column `polarity_ml`) with the manual classifications (in column `polarity`) we use three metrics: Recall, Precision, and F1.
The metrics inform us about the quality of the classifier. All three metrics range from 0 to 1. 


To calculate the three metrics, we need first to create three new columns via some recoding. 

The column `Positive_andRetrieved` includes a 1 if the manual coder and the classifier coded positive. = True positive
The column `Positive_notRetrieved` includes a 1 if the manual coder coded positive but the classifier coded negative. = False negative
The column `notPositive_butRetrieved` includes a 1 if the manual coder coded negative but the classifier coded 1. = False positive

```{r}

reviews_test$Positive_andRetrieved[reviews_test$polarity == "positive" & reviews_test$polarity_ml== "positive" ] <- 1
reviews_test$Positive_notRetrieved[reviews_test$polarity == "positive" & reviews_test$polarity_ml == "negative" ] <- 1
reviews_test$notPositive_butRetrieved[reviews_test$polarity == "negative" & reviews_test$polarity_ml == "positive" ] <- 1

```

### Recall 

By inspecting recall we can say how many positive reviews are retrieved by the classifier.
A recall of 1.0 means that our classifier retrieved all positive reviews. 
A recall of 0.8 means that our classifier retrieved 80% of all positive reviews. 

To obtain recall, we calculate:

```{r}

recall_pos <- (sum(reviews_test$Positive_andRetrieved, na.rm=TRUE))/(sum(reviews_test$Positive_notRetrieved, na.rm=TRUE) + (sum(reviews_test$Positive_andRetrieved, na.rm=TRUE)))
recall_pos


```


### Precision 

By inspecting precision we can say how many retrieved reviews are truely positive.
A precision of 1,0 means that all reviews retrieved by the classifier are truely positive. 
A precision of 0.8 means that 80% of the reviews that our classifier retrieved are truely positive reviews. 

To obtain precision, we calculate:

```{r}

precision_pos <- (sum(reviews_test$Positive_andRetrieved, na.rm=TRUE))/(sum(reviews_test$notPositive_butRetrieved, na.rm=TRUE) + (sum(reviews_test$Positive_andRetrieved, na.rm=TRUE)))
precision_pos # 

```


### F1

F1 is the harmonic mean between recall and precision. 

To obtain F1, we calculate:

```{r}

F1 <- (2 * precision_pos * recall_pos)/(precision_pos + recall_pos)
F1

```



