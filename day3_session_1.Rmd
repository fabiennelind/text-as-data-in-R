---
title: "Dictionaries"
author: "Fabienne Lind"
date: "November 15, 2022"
output:
  html_document:
    df_print: paged
---

# Automated Article Classification and Validation

### Data

For the next tasks, we will work with an example text data, headlines from German news articles about migration. The data set is a subset of the [REMINDER media corpus](https://doi.org/10.11587/IEGQ1B).

Let's load the data first and take a look. Each row represents one news article.

```{r}

articles <- read.csv("https://raw.githubusercontent.com/fabiennelind/Workshop_Multilingual-Text-Analysis_and_Comparative-Research/master/data/multilingual_data_annotated_translated.csv?token=AH2Y4HB34Z25K53J2X7QEV3BS7ADO")

```

For our exercise, we only work with the German headlines. Thus, we first need to apply a filter.
To select a subset of the data, we can use for example `subset`. We save the German subset in the dataframe object called `articles_de`.

```{r}

articles_de <- subset(articles, country == "Germany")

```

Let us now inspect the column `headline`, the column with our text to be analyzed. 

```{r}

head(articles_de$headline) # show the first lines of the column
class(articles_de$headline) # check the class of the column.
articles_de$headline <- as.character(articles_de$headline) # Change the class to character.
class(articles_de$headline) # check if the edit worked.

```

## Automated Classification with a Dictionary

For this tutorial, we like to identify all articles that mention political actors in their headlines. The salience of 'Political actors' is the concept that we like to measure with an automated text analysis method, a dictionary. As a first step, we define the concept more closely.

### Concept Definition

**Political actors** are here defined as political parties represented in the German Bundestag between 2000 and 2017, which is the period in which the articles in our sample where published. Next to these parties, we define German politicians with a leading role as political actors. To keep the task manageable for this exercise, we focus only on actors highly relevant between 2000 and 2017. 

We intend to measure the salience of political actors as simple binary variable:
1 = At least one political actor is mentioned
0 = No political actor is mentioned.

### Dictionary creation

A dictionary is a set of keywords or phrases that represent the concept of interest. 

We now start to collect relevant keywords for the dictionary. We start with a list of keywords that we consider most relevant. An example for a relevant keyword is "angela merkel".
For clarity, we here work with two keyword sets: we collect the keywords related to politicians in one vector (here named `politicians`), and keywords related to political parties in another vector (here named `parties`). 

The keywords are written as regular expressions. A ‘regular expression’ is a pattern that describes a string. To learn more about regular expressions, we recommend this R tutorial [(Wickham & Grolemund, 2017)](https://r4ds.had.co.nz/strings.html). To test regular expressions quickly, visit https://spannbaueradam.shinyapps.io/r_regex_tester/

```{r}

politicians <- c("angela merkel", "gerhard schröder", "(?:^|\\W)kanzler")
parties <- c("spd", "cdu", "grüne", "fdp", "linke", "afd")

```

Some questions:

Pros and cons of storing all keywords in one vector?
Should we also add "kanzlerin"?
What other keywords are relevant to measure the concept?


All vector names (e.g.,"politicians") are then saved in another vector.

```{r}

dict_name <- c("politicians", "parties") 

```

Before we search the keyword in the headlines, we  apply some pre-processing steps to the headlines. For this exercise, we designed the keywords all in lower case, so the headlines have to be lower case too.

```{r}

articles_de$headline <- tolower(articles_de$headline) # convert text to lower case
head(articles_de$headline)
```

We now search the keywords in the article headlines. The function `stri_count_regex` from the R package **stringr** can count how often a pattern appears in a text. We call this here the number of hits. The function can search for regular expression. We here ask to count a pattern in the column `headline` of the dataframe `articles_de`. 

The patterns to count are the politician keywords and the party keywords. 

```{r}
#install.packages("stringi")
library(stringi)

n <- length(dict_name)# number of keyword sets (each is counted separately)
codings <- vector("list", n)# create an empty list, to be filled in the loop

for (i in dict_name) {# each keyword set stored in vector 'dict name' is looked at separately
  print(i)
  match <- stri_count_regex(articles_de$headline, paste(get(i), collapse='|'))
  codings[[i]] <- data.frame(name=match)
}

codings <- codings[-c(1:n)] # save the relevant part of the list 
codings_df <- do.call("cbind", codings) # unlist
codings_df

```

Some recoding to get the column names correct.

```{r}

# replace names in resulting df with names from dict_name 
cols <- names(codings_df) == "name"# vector with all names in resulting dataframe
names(codings_df)[cols] <- paste0("name", seq.int(sum(cols))) # add a number behing each colname (to make them differ, necessary for next step)
oldnames <- colnames(codings_df)# a vector with the names of dict
newnames <- names(codings)# a vector with names stored in list

#install.packages("data.table")
library(data.table)
setnames(codings_df, old = oldnames, new = newnames)# finally replace the current names with the correct ones
codings_df

```

We now add the number of hits counted by the dictionary (saved in the dataframe `codings_df`) to the articles (the object `articles_de`). Since we did not shuffle the order of rows, we can bind both data frames together with the function `bind_cols`. We create a new dataframe with headlines, article meta-data and codings.

```{r}

#install.packages("dplyr")
library(dplyr)
articles_de_hits <- bind_cols(articles_de, codings_df)

```

One way to see the headlines for which the dictionary counted a hit?  

```{r}

testa <- subset(articles_de_hits, parties >=1)
head(testa$headline) 

testb <- subset(articles_de_hits, politicians >=1)
head(testb$headline) 

```

So far, we obtained a count, that represents how often the keywords were detected per text. Since weinitially proposed a simple binary measurement, we now do some recoding. 

We add a new column to the dataframe called `actors_d`. This column includes a 1 if at least one of all defined keywords creates a hit, and a 0 if no keyword was found. 

```{r}

articles_de_hits$actors_d <- case_when(articles_de_hits$parties >= 1 |articles_de_hits$politicians >= 1 ~ 1)# | means or. 
articles_de_hits <- articles_de_hits %>% mutate(actors_d = if_else(is.na(actors_d), 0, actors_d)) # set NA to 0 

```

According to our automated measurement, how many articles mention political actors in their headlines?

```{r}

table(articles_de_hits$actors_d) # descriptive overview

```

We have now managed to get an automated measurement for the variable. **But how valid is this measurement?** Does our small set of keywords represent the concept adequately?

A common procedure in automated content analysis is to test construct validity. We ask:
How close is this automated measurement to a more trusted measurement: Human understanding of text.
Let's put this to practice. 

## Dictionary validation with a human coded baseline

To validate the dictionary, we compare the classifications of the dictionary with the classifications of human coders. 

We create the human coded baseline together. 

### Intercoder reliability test

To ensure the quality of our manual coding, we first perform an intercoder reliability test. For this tutorial, we select a random set of 10 articles. In a real study the number of observations coded by several coders should be higher.  

```{r}

set.seed(57)# setting a seed ensures that the random selection can be repeated in the same way
library(dplyr)
intercoder_set <- sample_n(articles_de, 10) #select 10 random rows

```

We now add an empty column called `actors_m`, so that coders can enter the manual codes.

```{r}

intercoder_set$actors_m <- "" 

```

We then create several duplicates of the intercoder reliability set, one for each coder. We create separate files so that coders can so that everyone codes individually and does not peek by mistake.
To each of these sets we add the coder name in a new column called `coder_name`.
For this example, we now need 2 volunteers. Who would like to code?

```{r}

intercoder_set_coder1 <- intercoder_set
intercoder_set_coder1$coder_name <- "Coder1"

intercoder_set_coder2 <- intercoder_set
intercoder_set_coder2$coder_name <- "Coder2"

#...

```

We then want to save the data sets in google sheets. Detailed instructions about the conncection of **R** and **Google Sheets** can be found in  [this](https://googlesheets4.tidyverse.org/articles/drive-and-sheets.html) and [this ](https://googlesheets4.tidyverse.org/articles/drive-and-sheets.html) tutorial.

The two packages needed here are **googledrive** and **googlesheets4**.

```{r}

#install.packages("googledrive")
#install.packages("googlesheets4")
library(googledrive)
library(googlesheets4)

```

```{r}

# Authentication
drive_auth(email ="fabienne.lind@gmail.com")
gs4_auth(token = drive_token())
drive_user()

```

We now save the datasets for the intercoder reliability test as Google Sheets with the function `gs4_create`. 

```{r}

sheet_id1<- gs4_create("intercoder_set_coder1",sheets = intercoder_set_coder1)
sheet_id2<- gs4_create("intercoder_set_coder2",sheets = intercoder_set_coder2)

```

Ready to code? We will post links for the different files. Read the column `headline`. If the headline mentions a political actor insert `1` in the column `actors_m`. Enter a `0` in `actors_m` if the headline does not mention a political actor.

After you finished coding, we read all sheets back into Rstudio (now with manual classifications for `actors_m`).

```{r}

intercoder_set_coder1c <- read_sheet(sheet_id1)
intercoder_set_coder2c <- read_sheet(sheet_id2)


```

All dataframes are combined into one dataframe with the function `rbind`.

```{r}

reliability_set <- rbind(intercoder_set_coder1c, intercoder_set_coder2c) 

```

Too calculate the agreement between coders, we first restructure the `reliability_set` a bit (the different coders become variables). 'id' is the name of our id variable. 'coder_name' is the column with the different coder ids. And 'actors_m' is the variable for which we seek to test intercoder reliability.

```{r}

#install.packages("reshape2")
library(reshape2) 

reliability_restructured <- dcast(reliability_set, id ~ coder_name, value.var="actors_m")

reliability_transp <- t(reliability_restructured) # transpose data frames (rows to columns, columns to rows)
reliability_matrix <- data.matrix(reliability_transp) # convert df.t to matrix 
reliability_matrix_final <- reliability_matrix[-1,] # delete first row of matrix

```

The package **irr** allows to calculate various coefficients of intercoder reliability. 
We calculate Krippendorff's alpha for this example.

```{r}

#install.packages("irr")
library(irr)  

alpha_de <- kripp.alpha(reliability_matrix_final, method ="nominal") # select the appropriate method, nominal is default,
alpha_de

```

If alpha is large enough, we consider the quality of our manual coding as sufficient. We can then start with the creation of a larger manual baseline to be compared with the dictionary classifications.

## Creating a manually coded baseline

We pick 100 headlines randomly. 

```{r}

#install.packages("dplyr")
library(dplyr)

set.seed(789)# setting a seed ensures that the random selection can be repeated in the same way
manual_set <- sample_n(articles_de, 100)# select 100 random rows

```

We add again an empty column called `actors_m`, for coders to enter the manual codes. This time, we also add an empty column for the coder names.

```{r}

manual_set$actors_m <- "" 
manual_set$coder_name <- ""

```

We create a google sheet for the task with `gs4_create`. 

```{r}

sheet_id_manual<- gs4_create("manual_set", sheets = manual_set)

```

Please open the sheet in your browser. Enter a coding name (free to pick) in the column `coder_name` for a couple of rows first. Then start to enter 1 (political actor in headline mentioned) or 0 (not mentioned) in the column `actors_m` for the rows with your coding name. Our goal is to finish coding of all headlines.


After you finish coding, we read all sheets back into Rstudio (now with manual classifications for `actors_m`).

```{r}

manual_set_coded <- read_sheet(sheet_id_manual)

```

We need to create a data set, where the manual and automated classifications are included.

```{r}

manual_set_coded <- subset(manual_set_coded, select = c("id", "actors_m"))# we need only 2 columns from the manual set
articles_coded_d_m <- merge(manual_set_coded, articles_de_hits, by ="id")
                           
```

## Compare automated with manual classifications 

We compare the automated classification (in column `actors_d`) with the manual classifications (in column `actors_m`) we use three metrics: Recall, Precision, and F1.
The metrics inform us about the quality of the dictionary. All three metrics range from 0 to 1. 
We assume that our manual classification identified all relevant articles (here: headlines that mention a political actor).


To calculate the three metrics, we need first to create three new columns via some recoding. 

The column `Relevant_andRetrieved` includes a 1 if the manual coder and the dictionary coded 1. = True positive
The column `Relevant_notRetrieved` includes a 1 if the manual coder coded 1 but the dictionary coded 0. = False negative
The column `notRelevant_butRetrieved` includes a 1 if the manual coder coded 0 but the dictionary coded 1. = False positive

```{r}

articles_coded_d_m$Relevant_andRetrieved[articles_coded_d_m$actors_m == 1 & articles_coded_d_m$actors_d== 1 ] <- 1
articles_coded_d_m$Relevant_notRetrieved[articles_coded_d_m$actors_m == 1 & articles_coded_d_m$actors_d == 0 ] <- 1
articles_coded_d_m$notRelevant_butRetrieved[articles_coded_d_m$actors_m == 0 & articles_coded_d_m$actors_d == 1 ] <- 1

```

### Recall 

By inspecting recall we can say how many relevant articles are retrieved by the dictionary.
A recall of 1.0 means that our dictionary retrieved all relevant articles. 
A recall of 0.8 means that our dictionary retrieved 80% of all relevant articles. 

To obtain recall, we calculate:

```{r}

recall <- (sum(articles_coded_d_m$Relevant_andRetrieved, na.rm=TRUE))/(sum(articles_coded_d_m$Relevant_notRetrieved, na.rm=TRUE) + (sum(articles_coded_d_m$Relevant_andRetrieved, na.rm=TRUE)))
recall


```


### Precision 

By inspecting precision we can say how many retrieved articles are relevant.
A precision of 1,0 means that all articles retrieved by the dictionary are relevant. 
A precision of 0.8 means that 80% of the articles that our dictionary retrieved are relevant articles. 

To obtain precision, we calculate:

```{r}

precision <- (sum(articles_coded_d_m$Relevant_andRetrieved, na.rm=TRUE))/(sum(articles_coded_d_m$notRelevant_butRetrieved, na.rm=TRUE) + (sum(articles_coded_d_m$Relevant_andRetrieved, na.rm=TRUE)))
precision # 

```


### F1

F1 is the harmonic mean between recall and precision. 

To obtain F1, we calculate:

```{r}

F1 <- (2 * precision * recall)/(precision + recall)
F1

```

Questions: 

- Say we have a precision of .9 but a recall of .1, what does this mean for the quality of our dictionary?

- What can we do to improve recall?

- What can we do to improve precision?



### Improving precision: Fine-tuning of the keywords.

```{r}

politicians <- c()
parties <- c()

```

### Improving recall: Extending the keywords

```{r}


```


#Further readings

[Song et al. (2020)](https://doi.org/10.1080/10584609.2020.1723752)


